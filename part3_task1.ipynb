{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "kZrgZQkETPFK"
   },
   "outputs": [],
   "source": [
    "# Can the network fit random labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_oD5ooYMxEuu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "EVQhDvADxSma"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "-q2k3yOexhg0"
   },
   "outputs": [],
   "source": [
    "# Load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Y4_VsZjXzFlv"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_size = 784  # MNIST image size (28x28) flattened\n",
    "num_classes = 10  # Number of output classes for MNIST (digits 0-9)\n",
    "num_epochs = 1000\n",
    "batch_size = 64\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "_BHT6NvSxrAb"
   },
   "outputs": [],
   "source": [
    "# Shuffle labels in the train dataset\n",
    "np.random.seed(42)\n",
    "# random_labels = np.random.permutation(len(train_dataset.targets))\n",
    "random_labels = np.random.choice(10, len(train_dataset.targets)) # generate random labels between 0 and 9\n",
    "train_dataset.targets = torch.Tensor(random_labels).long()\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "aNVjQSBwxvJ2"
   },
   "outputs": [],
   "source": [
    "class DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, input_size)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "9ATfkoHoxxE0"
   },
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = DNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "2iUu-rJlEOUd"
   },
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, optimizer, num_epochs):\n",
    "    model.train()\n",
    "    loss_list = []\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Forward pass\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Add loss for each batch to total loss for the epoch\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        loss_list.append(avg_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "9fbHWs3vESYv"
   },
   "outputs": [],
   "source": [
    "# Test function\n",
    "def test(model, optimizer):\n",
    "    model.eval() \n",
    "    loss_list = []\n",
    "    with torch.no_grad():\n",
    "      for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        loss_list.append(avg_loss)\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k08gwrzxQUot",
    "outputId": "58f04df6-c91d-4cbe-d7c5-e497a4287064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 2.3037\n",
      "Epoch [2/1000], Loss: 2.3029\n",
      "Epoch [3/1000], Loss: 2.3023\n",
      "Epoch [4/1000], Loss: 2.3016\n",
      "Epoch [5/1000], Loss: 2.3001\n",
      "Epoch [6/1000], Loss: 2.2975\n",
      "Epoch [7/1000], Loss: 2.2936\n",
      "Epoch [8/1000], Loss: 2.2873\n",
      "Epoch [9/1000], Loss: 2.2795\n",
      "Epoch [10/1000], Loss: 2.2691\n",
      "Epoch [11/1000], Loss: 2.2561\n",
      "Epoch [12/1000], Loss: 2.2406\n",
      "Epoch [13/1000], Loss: 2.2234\n",
      "Epoch [14/1000], Loss: 2.2034\n",
      "Epoch [15/1000], Loss: 2.1827\n",
      "Epoch [16/1000], Loss: 2.1618\n",
      "Epoch [17/1000], Loss: 2.1391\n",
      "Epoch [18/1000], Loss: 2.1177\n",
      "Epoch [19/1000], Loss: 2.0934\n",
      "Epoch [20/1000], Loss: 2.0694\n",
      "Epoch [21/1000], Loss: 2.0462\n",
      "Epoch [22/1000], Loss: 2.0229\n",
      "Epoch [23/1000], Loss: 1.9997\n",
      "Epoch [24/1000], Loss: 1.9769\n",
      "Epoch [25/1000], Loss: 1.9538\n",
      "Epoch [26/1000], Loss: 1.9342\n",
      "Epoch [27/1000], Loss: 1.9128\n",
      "Epoch [28/1000], Loss: 1.8903\n",
      "Epoch [29/1000], Loss: 1.8700\n",
      "Epoch [30/1000], Loss: 1.8502\n",
      "Epoch [31/1000], Loss: 1.8327\n",
      "Epoch [32/1000], Loss: 1.8118\n",
      "Epoch [33/1000], Loss: 1.7963\n",
      "Epoch [34/1000], Loss: 1.7765\n",
      "Epoch [35/1000], Loss: 1.7613\n",
      "Epoch [36/1000], Loss: 1.7435\n",
      "Epoch [37/1000], Loss: 1.7277\n",
      "Epoch [38/1000], Loss: 1.7111\n",
      "Epoch [39/1000], Loss: 1.6966\n",
      "Epoch [40/1000], Loss: 1.6816\n",
      "Epoch [41/1000], Loss: 1.6678\n",
      "Epoch [42/1000], Loss: 1.6539\n",
      "Epoch [43/1000], Loss: 1.6408\n",
      "Epoch [44/1000], Loss: 1.6280\n",
      "Epoch [45/1000], Loss: 1.6170\n",
      "Epoch [46/1000], Loss: 1.6019\n",
      "Epoch [47/1000], Loss: 1.5888\n",
      "Epoch [48/1000], Loss: 1.5798\n",
      "Epoch [49/1000], Loss: 1.5671\n",
      "Epoch [50/1000], Loss: 1.5564\n",
      "Epoch [51/1000], Loss: 1.5453\n",
      "Epoch [52/1000], Loss: 1.5316\n",
      "Epoch [53/1000], Loss: 1.5274\n",
      "Epoch [54/1000], Loss: 1.5150\n",
      "Epoch [55/1000], Loss: 1.5034\n",
      "Epoch [56/1000], Loss: 1.4930\n",
      "Epoch [57/1000], Loss: 1.4861\n",
      "Epoch [58/1000], Loss: 1.4765\n",
      "Epoch [59/1000], Loss: 1.4712\n",
      "Epoch [60/1000], Loss: 1.4578\n",
      "Epoch [61/1000], Loss: 1.4508\n",
      "Epoch [62/1000], Loss: 1.4420\n",
      "Epoch [63/1000], Loss: 1.4347\n",
      "Epoch [64/1000], Loss: 1.4280\n",
      "Epoch [65/1000], Loss: 1.4173\n",
      "Epoch [66/1000], Loss: 1.4121\n",
      "Epoch [67/1000], Loss: 1.4037\n",
      "Epoch [68/1000], Loss: 1.3945\n",
      "Epoch [69/1000], Loss: 1.3921\n",
      "Epoch [70/1000], Loss: 1.3823\n",
      "Epoch [71/1000], Loss: 1.3779\n",
      "Epoch [72/1000], Loss: 1.3671\n",
      "Epoch [73/1000], Loss: 1.3614\n",
      "Epoch [74/1000], Loss: 1.3593\n",
      "Epoch [75/1000], Loss: 1.3525\n",
      "Epoch [76/1000], Loss: 1.3448\n",
      "Epoch [77/1000], Loss: 1.3416\n",
      "Epoch [78/1000], Loss: 1.3331\n",
      "Epoch [79/1000], Loss: 1.3262\n",
      "Epoch [80/1000], Loss: 1.3205\n",
      "Epoch [81/1000], Loss: 1.3175\n",
      "Epoch [82/1000], Loss: 1.3130\n",
      "Epoch [83/1000], Loss: 1.3093\n",
      "Epoch [84/1000], Loss: 1.3009\n",
      "Epoch [85/1000], Loss: 1.2954\n",
      "Epoch [86/1000], Loss: 1.2879\n",
      "Epoch [87/1000], Loss: 1.2878\n",
      "Epoch [88/1000], Loss: 1.2877\n",
      "Epoch [89/1000], Loss: 1.2758\n",
      "Epoch [90/1000], Loss: 1.2695\n",
      "Epoch [91/1000], Loss: 1.2681\n",
      "Epoch [92/1000], Loss: 1.2651\n",
      "Epoch [93/1000], Loss: 1.2551\n",
      "Epoch [94/1000], Loss: 1.2517\n",
      "Epoch [95/1000], Loss: 1.2553\n",
      "Epoch [96/1000], Loss: 1.2447\n",
      "Epoch [97/1000], Loss: 1.2416\n",
      "Epoch [98/1000], Loss: 1.2392\n",
      "Epoch [99/1000], Loss: 1.2331\n",
      "Epoch [100/1000], Loss: 1.2287\n",
      "Epoch [101/1000], Loss: 1.2219\n",
      "Epoch [102/1000], Loss: 1.2226\n",
      "Epoch [103/1000], Loss: 1.2179\n",
      "Epoch [104/1000], Loss: 1.2161\n",
      "Epoch [105/1000], Loss: 1.2089\n",
      "Epoch [106/1000], Loss: 1.2015\n",
      "Epoch [107/1000], Loss: 1.2042\n",
      "Epoch [108/1000], Loss: 1.1974\n",
      "Epoch [109/1000], Loss: 1.1999\n",
      "Epoch [110/1000], Loss: 1.1954\n",
      "Epoch [111/1000], Loss: 1.1898\n",
      "Epoch [112/1000], Loss: 1.1881\n",
      "Epoch [113/1000], Loss: 1.1848\n",
      "Epoch [114/1000], Loss: 1.1792\n",
      "Epoch [115/1000], Loss: 1.1769\n",
      "Epoch [116/1000], Loss: 1.1675\n",
      "Epoch [117/1000], Loss: 1.1734\n",
      "Epoch [118/1000], Loss: 1.1725\n",
      "Epoch [119/1000], Loss: 1.1643\n",
      "Epoch [120/1000], Loss: 1.1608\n",
      "Epoch [121/1000], Loss: 1.1561\n",
      "Epoch [122/1000], Loss: 1.1523\n",
      "Epoch [123/1000], Loss: 1.1538\n",
      "Epoch [124/1000], Loss: 1.1589\n",
      "Epoch [125/1000], Loss: 1.1471\n",
      "Epoch [126/1000], Loss: 1.1391\n",
      "Epoch [127/1000], Loss: 1.1500\n",
      "Epoch [128/1000], Loss: 1.1433\n",
      "Epoch [129/1000], Loss: 1.1376\n",
      "Epoch [130/1000], Loss: 1.1320\n",
      "Epoch [131/1000], Loss: 1.1324\n",
      "Epoch [132/1000], Loss: 1.1279\n",
      "Epoch [133/1000], Loss: 1.1263\n",
      "Epoch [134/1000], Loss: 1.1259\n",
      "Epoch [135/1000], Loss: 1.1249\n",
      "Epoch [136/1000], Loss: 1.1139\n",
      "Epoch [137/1000], Loss: 1.1275\n",
      "Epoch [138/1000], Loss: 1.1189\n",
      "Epoch [139/1000], Loss: 1.1174\n",
      "Epoch [140/1000], Loss: 1.1099\n",
      "Epoch [141/1000], Loss: 1.1040\n",
      "Epoch [142/1000], Loss: 1.1107\n",
      "Epoch [143/1000], Loss: 1.1039\n",
      "Epoch [144/1000], Loss: 1.0996\n",
      "Epoch [145/1000], Loss: 1.1070\n",
      "Epoch [146/1000], Loss: 1.0986\n",
      "Epoch [147/1000], Loss: 1.0874\n",
      "Epoch [148/1000], Loss: 1.1023\n",
      "Epoch [149/1000], Loss: 1.0906\n",
      "Epoch [150/1000], Loss: 1.0896\n",
      "Epoch [151/1000], Loss: 1.0882\n",
      "Epoch [152/1000], Loss: 1.0970\n",
      "Epoch [153/1000], Loss: 1.0712\n",
      "Epoch [154/1000], Loss: 1.0922\n",
      "Epoch [155/1000], Loss: 1.0799\n",
      "Epoch [156/1000], Loss: 1.0819\n",
      "Epoch [157/1000], Loss: 1.0761\n",
      "Epoch [158/1000], Loss: 1.0694\n",
      "Epoch [159/1000], Loss: 1.0718\n",
      "Epoch [160/1000], Loss: 1.0687\n",
      "Epoch [161/1000], Loss: 1.0681\n",
      "Epoch [162/1000], Loss: 1.0726\n",
      "Epoch [163/1000], Loss: 1.0637\n",
      "Epoch [164/1000], Loss: 1.0642\n",
      "Epoch [165/1000], Loss: 1.0677\n",
      "Epoch [166/1000], Loss: 1.0591\n",
      "Epoch [167/1000], Loss: 1.0597\n",
      "Epoch [168/1000], Loss: 1.0591\n",
      "Epoch [169/1000], Loss: 1.0574\n",
      "Epoch [170/1000], Loss: 1.0439\n",
      "Epoch [171/1000], Loss: 1.0551\n",
      "Epoch [172/1000], Loss: 1.0531\n",
      "Epoch [173/1000], Loss: 1.0523\n",
      "Epoch [174/1000], Loss: 1.0372\n",
      "Epoch [175/1000], Loss: 1.0453\n",
      "Epoch [176/1000], Loss: 1.0492\n",
      "Epoch [177/1000], Loss: 1.0405\n",
      "Epoch [178/1000], Loss: 1.0401\n",
      "Epoch [179/1000], Loss: 1.0354\n",
      "Epoch [180/1000], Loss: 1.0337\n",
      "Epoch [181/1000], Loss: 1.0355\n",
      "Epoch [182/1000], Loss: 1.0390\n",
      "Epoch [183/1000], Loss: 1.0353\n",
      "Epoch [184/1000], Loss: 1.0224\n",
      "Epoch [185/1000], Loss: 1.0270\n",
      "Epoch [186/1000], Loss: 1.0248\n",
      "Epoch [187/1000], Loss: 1.0362\n",
      "Epoch [188/1000], Loss: 1.0219\n",
      "Epoch [189/1000], Loss: 1.0234\n",
      "Epoch [190/1000], Loss: 1.0225\n",
      "Epoch [191/1000], Loss: 1.0148\n",
      "Epoch [192/1000], Loss: 1.0239\n",
      "Epoch [193/1000], Loss: 1.0101\n",
      "Epoch [194/1000], Loss: 1.0163\n",
      "Epoch [195/1000], Loss: 1.0245\n",
      "Epoch [196/1000], Loss: 1.0138\n",
      "Epoch [197/1000], Loss: 1.0063\n",
      "Epoch [198/1000], Loss: 1.0071\n",
      "Epoch [199/1000], Loss: 1.0147\n",
      "Epoch [200/1000], Loss: 1.0117\n",
      "Epoch [201/1000], Loss: 1.0038\n",
      "Epoch [202/1000], Loss: 1.0042\n",
      "Epoch [203/1000], Loss: 1.0113\n",
      "Epoch [204/1000], Loss: 1.0079\n",
      "Epoch [205/1000], Loss: 1.0005\n",
      "Epoch [206/1000], Loss: 1.0035\n",
      "Epoch [207/1000], Loss: 0.9957\n",
      "Epoch [208/1000], Loss: 0.9915\n",
      "Epoch [209/1000], Loss: 0.9928\n",
      "Epoch [210/1000], Loss: 0.9970\n",
      "Epoch [211/1000], Loss: 1.0036\n",
      "Epoch [212/1000], Loss: 0.9952\n",
      "Epoch [213/1000], Loss: 0.9900\n",
      "Epoch [214/1000], Loss: 0.9936\n",
      "Epoch [215/1000], Loss: 0.9876\n",
      "Epoch [216/1000], Loss: 0.9803\n",
      "Epoch [217/1000], Loss: 0.9865\n",
      "Epoch [218/1000], Loss: 0.9965\n",
      "Epoch [219/1000], Loss: 0.9853\n",
      "Epoch [220/1000], Loss: 0.9778\n",
      "Epoch [221/1000], Loss: 0.9792\n",
      "Epoch [222/1000], Loss: 0.9819\n",
      "Epoch [223/1000], Loss: 0.9910\n",
      "Epoch [224/1000], Loss: 0.9894\n",
      "Epoch [225/1000], Loss: 0.9691\n",
      "Epoch [226/1000], Loss: 0.9741\n",
      "Epoch [227/1000], Loss: 0.9850\n",
      "Epoch [228/1000], Loss: 0.9646\n",
      "Epoch [229/1000], Loss: 0.9800\n",
      "Epoch [230/1000], Loss: 0.9803\n",
      "Epoch [231/1000], Loss: 0.9629\n",
      "Epoch [232/1000], Loss: 0.9671\n",
      "Epoch [233/1000], Loss: 0.9776\n",
      "Epoch [234/1000], Loss: 0.9648\n",
      "Epoch [235/1000], Loss: 0.9672\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "train_losses = train(model, optimizer, num_epochs)\n",
    "test_losses = test(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train_losses.shape: {len(train_losses)}\")\n",
    "print(f\"test_losses.shape: {len(test_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEqQiOIVx3Ah"
   },
   "outputs": [],
   "source": [
    "# Plot the training and testing loss\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs+1), test_losses, label='Testing Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Testing Loss vs Epochs')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
